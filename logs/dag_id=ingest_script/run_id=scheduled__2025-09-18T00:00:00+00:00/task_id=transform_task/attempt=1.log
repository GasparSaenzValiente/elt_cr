{"timestamp":"2025-09-18T15:18:28.138841","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T15:18:28.139211","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T15:18:28.317322Z","level":"error","event":"JAVA_HOME is not set","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:18:28.402495","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":8,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":111,"name":"launch_gateway"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T15:28:00.372507","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T15:28:00.372852","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T15:28:00.763196Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.647397Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.701897Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.702039Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.704948Z","level":"error","event":"org.apache.hadoop#hadoop-aws added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.705124Z","level":"error","event":"com.amazonaws#aws-java-sdk-bundle added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.705664Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-943efdb3-6fca-4361-ad51-61ee8e5ddabb;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:01.705798Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:07.495928Z","level":"error","event":"\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:07.667338Z","level":"error","event":"downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.414551Z","level":"error","event":"\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (23910ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415169Z","level":"error","event":":: resolution report :: resolve 5797ms :: artifacts dl 23913ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415375Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415461Z","level":"error","event":"\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415582Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415682Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.415789Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.416147Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.416421Z","level":"error","event":"\t|      default     |   2   |   1   |   1   |   0   ||   1   |   1   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.416546Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418036Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418146Z","level":"error","event":":: problems summary ::","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418190Z","level":"error","event":":::: WARNINGS","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418274Z","level":"error","event":"\tproblem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.pom: Downloaded file size (0) doesn't match expected Content Length (25377) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.pom. Please retry. (215ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418319Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418353Z","level":"error","event":"\t\tmodule not found: org.apache.hadoop#hadoop-aws;3.3.2","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418387Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418417Z","level":"error","event":"\t==== local-m2-cache: tried","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418447Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418475Z","level":"error","event":"\t  file:/home/airflow/.m2/repository/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.pom","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418504Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418532Z","level":"error","event":"\t  -- artifact org.apache.hadoop#hadoop-aws;3.3.2!hadoop-aws.jar:","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418560Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418586Z","level":"error","event":"\t  file:/home/airflow/.m2/repository/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418619Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418656Z","level":"error","event":"\t==== local-ivy-cache: tried","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418679Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418707Z","level":"error","event":"\t  /home/airflow/.ivy2.5.2/local/org.apache.hadoop/hadoop-aws/3.3.2/ivys/ivy.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418732Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418759Z","level":"error","event":"\t  -- artifact org.apache.hadoop#hadoop-aws;3.3.2!hadoop-aws.jar:","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418784Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418812Z","level":"error","event":"\t  /home/airflow/.ivy2.5.2/local/org.apache.hadoop/hadoop-aws/3.3.2/jars/hadoop-aws.jar","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418843Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418870Z","level":"error","event":"\t==== central: tried","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418894Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418917Z","level":"error","event":"\t  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.pom","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418942Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418966Z","level":"error","event":"\t==== spark-packages: tried","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.418990Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419016Z","level":"error","event":"\t  https://repos.spark-packages.org/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.pom","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419042Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419066Z","level":"error","event":"\t  -- artifact org.apache.hadoop#hadoop-aws;3.3.2!hadoop-aws.jar:","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419091Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419115Z","level":"error","event":"\t  https://repos.spark-packages.org/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419139Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419163Z","level":"error","event":"\t\t::::::::::::::::::::::::::::::::::::::::::::::","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419188Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419211Z","level":"error","event":"\t\t::          UNRESOLVED DEPENDENCIES         ::","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419233Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419256Z","level":"error","event":"\t\t::::::::::::::::::::::::::::::::::::::::::::::","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419283Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419307Z","level":"error","event":"\t\t:: org.apache.hadoop#hadoop-aws;3.3.2: not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419332Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419357Z","level":"error","event":"\t\t::::::::::::::::::::::::::::::::::::::::::::::","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419386Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419412Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419438Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419461Z","level":"error","event":":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419510Z","level":"error","event":"Exception in thread \"main\" java.lang.RuntimeException: [unresolved dependency: org.apache.hadoop#hadoop-aws;3.3.2: not found]","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419539Z","level":"error","event":"\tat org.apache.spark.util.MavenUtils$.resolveMavenCoordinates(MavenUtils.scala:540)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419564Z","level":"error","event":"\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:123)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419591Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:341)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419619Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419646Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419672Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419711Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419736Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419762Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.419790Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:28:31.476885","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":8,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":111,"name":"launch_gateway"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T15:39:35.006317","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T15:39:35.006608","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T15:39:36.176160Z","level":"error","event":"25/09/18 15:39:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:39:36.296546Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:39:36.296685Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:39:38.413785","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o29.json.\n: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/impl/prefetch/PrefetchingStatistics\n\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)\n\tat java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:524)\n\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:427)\n\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:421)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:420)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:519)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 36 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":16,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":425,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T15:47:42.613190","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T15:47:42.613542","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T15:47:44.288960Z","level":"error","event":"25/09/18 15:47:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:44.420899Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:44.421200Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:45.098399Z","level":"error","event":"25/09/18 15:47:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.440928Z","level":"error","event":"25/09/18 15:47:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992157Z","level":"error","event":"25/09/18 15:47:46 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://raw-data/players.json.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992313Z","level":"error","event":"org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://raw-data/players.json': getFileStatus on s3a://raw-data/players.json: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB44B0ED3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB44B0ED3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992409Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992485Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992540Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992596Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992649Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992707Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992755Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992808Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992861Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992916Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.992973Z","level":"error","event":"\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993030Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993086Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993141Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993197Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993253Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993308Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993364Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993425Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993481Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993543Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993598Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993663Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993728Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993788Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993843Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993902Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.993963Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994026Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994089Z","level":"error","event":"Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB44B0ED3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994151Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994214Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994275Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994341Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994475Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994551Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994613Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994673Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994736Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994776Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994832Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994898Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.994955Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995013Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995073Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995139Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995206Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995273Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995338Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995400Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995463Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:46.995526Z","level":"error","event":"\t... 26 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T15:47:47.040538","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o33.json.\n: org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://raw-data/players.json': getFileStatus on s3a://raw-data/players.json: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB704A0D3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB704A0D3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18666B1AB704A0D3; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)\n\t... 23 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":22,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":425,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:13:12.855738","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:13:12.856054","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:13:14.481576Z","level":"error","event":"25/09/18 21:13:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:14.630217Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:14.630568Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:15.253254Z","level":"error","event":"25/09/18 21:13:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:16.658806Z","level":"error","event":"25/09/18 21:13:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.283800Z","level":"error","event":"25/09/18 21:13:17 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://raw-data/players.json.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.283986Z","level":"error","event":"org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://raw-data/players.json': getFileStatus on s3a://raw-data/players.json: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF44D4979; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF44D4979; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284057Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284109Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284155Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284206Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284259Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284310Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284351Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284386Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284422Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284461Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284490Z","level":"error","event":"\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284518Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284554Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284585Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284615Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284646Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284678Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284709Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284739Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284771Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284803Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284836Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284866Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284896Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284925Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284953Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.284983Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285016Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285046Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285076Z","level":"error","event":"Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF44D4979; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285106Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285135Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285162Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285190Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285267Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285303Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285332Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285361Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285393Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285422Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285450Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285484Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285513Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285543Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285574Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285604Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285637Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285669Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285699Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285728Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285757Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.285786Z","level":"error","event":"\t... 26 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:13:17.331598","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o33.json.\n: org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://raw-data/players.json': getFileStatus on s3a://raw-data/players.json: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF7038CE7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF7038CE7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667CDDF7038CE7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)\n\t... 23 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":22,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":425,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:19:14.589983","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:19:14.590314","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:19:15.719105Z","level":"error","event":"25/09/18 21:19:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:19:15.847719Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:19:15.847929Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:19:17.875848Z","level":"error","event":"25/09/18 21:19:17 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:19:18.804021","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AnalysisException","exc_value":"[PATH_NOT_FOUND] Path does not exist: s3a://cr-raw-data/players.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":22,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":425,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:22:50.519734","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:22:50.520060","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:22:52.130956Z","level":"error","event":"25/09/18 21:22:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:52.257256Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:52.257550Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:52.883072Z","level":"error","event":"25/09/18 21:22:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:54.296990Z","level":"error","event":"25/09/18 21:22:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.297811Z","level":"error","event":"25/09/18 21:22:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752476Z","level":"info","event":"+--------------------+--------------------+--------------------+-----------+----------------------------+------------+--------------------+-----------------+----------------+--------------------+------------------+--------------------+-----------------------+--------------------+-------------------------------+---------+-----------------+--------+---------+----------------------------+--------------------+-------------------------+------+----------+--------------------+------+----------+--------------------+----------+--------------+--------------+--------------+---------------------+------------------+--------+----------+----+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752675Z","level":"info","event":"|        achievements|               arena|              badges|battleCount|bestPathOfLegendSeasonResult|bestTrophies|               cards|challengeCardsWon|challengeMaxWins|                clan|clanCardsCollected|         currentDeck|currentDeckSupportCards|currentFavouriteCard|currentPathOfLegendSeasonResult|donations|donationsReceived|expLevel|expPoints|lastPathOfLegendSeasonResult|    leagueStatistics|legacyTrophyRoadHighScore|losses|      name|            progress|  role|starPoints|        supportCards|       tag|threeCrownWins|totalDonations|totalExpPoints|tournamentBattleCount|tournamentCardsWon|trophies|warDayWins|wins|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752766Z","level":"info","event":"+--------------------+--------------------+--------------------+-----------+----------------------------+------------+--------------------+-----------------+----------------+--------------------+------------------+--------------------+-----------------------+--------------------+-------------------------------+---------+-----------------+--------+---------+----------------------------+--------------------+-------------------------+------+----------+--------------------+------+----------+--------------------+----------+--------------+--------------+--------------+---------------------+------------------+--------+----------+----+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752833Z","level":"info","event":"|[{NULL, Join a Cl...|{54000031, Legend...|[{{https://api-as...|       2490|                {2, NULL, 0}|        9878|[{113, 5, NULL, {...|             1983|              10|{16000013, cordob...|             26854|[{0, 3, 1, {https...|   [{0, {https://api...|{4, {https://api-...|                   {1, NULL, 0}|       16|              120|      46|     7279|                {1, NULL, 0}|{{2021-12, 5534},...|                     5534|  1041|RL|{{{168000029, Sil...|member|     76505|[{0, {https://api...|#2PPCJ0UUP|           840|          8526|        311049|                   52|                31|    9878|        10|1449|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752907Z","level":"info","event":"+--------------------+--------------------+--------------------+-----------+----------------------------+------------+--------------------+-----------------+----------------+--------------------+------------------+--------------------+-----------------------+--------------------+-------------------------------+---------+-----------------+--------+---------+----------------------------+--------------------+-------------------------+------+----------+--------------------+------+----------+--------------------+----------+--------------+--------------+--------------+---------------------+------------------+--------+----------+----+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.752980Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:22:57.927378","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AnalysisException","exc_value":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `score` cannot be resolved. Did you mean one of the following? [`role`, `cards`, `clan`, `name`, `tag`].;\n'Project [name#31, 'score]\n+- Relation [achievements#8,arena#9,badges#10,battleCount#11L,bestPathOfLegendSeasonResult#12,bestTrophies#13L,cards#14,challengeCardsWon#15L,challengeMaxWins#16L,clan#17,clanCardsCollected#18L,currentDeck#19,currentDeckSupportCards#20,currentFavouriteCard#21,currentPathOfLegendSeasonResult#22,donations#23L,donationsReceived#24L,expLevel#25L,expPoints#26L,lastPathOfLegendSeasonResult#27,leagueStatistics#28,legacyTrophyRoadHighScore#29L,losses#30L,name#31,... 13 more fields] json\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":25,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py","lineno":3227,"name":"select"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:53:22.274115","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:53:22.274417","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:53:23.479059Z","level":"error","event":"25/09/18 21:53:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:53:23.612027Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:53:23.612377Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:53:25.657576Z","level":"error","event":"25/09/18 21:53:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.015628Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.015827Z","level":"info","event":"|      name|bestTrophies|         currentDeck|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.015914Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.015990Z","level":"info","event":"|RL|        9878|[{0, 3, 1, {https...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.016059Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.016128Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:53:29.132268","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o44.json.\n: org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://processed-data/players_transformed': getFileStatus on s3a://processed-data/players_transformed: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F0F804666C0; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F0F804666C0; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F0F804666C0; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)\n\t... 49 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":28,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1658,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:55:57.942142","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:55:57.942550","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:55:59.191456Z","level":"error","event":"25/09/18 21:55:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:55:59.334587Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:55:59.334768Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:55:59.968879Z","level":"error","event":"25/09/18 21:55:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:56:01.288889Z","level":"error","event":"25/09/18 21:56:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465280Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465510Z","level":"info","event":"|      name|bestTrophies|         currentDeck|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465617Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465705Z","level":"info","event":"|RL|        9878|[{0, 3, 1, {https...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465795Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.465878Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:56:04.623243","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o44.json.\n: org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://processed-data/players_transformed': getFileStatus on s3a://processed-data/players_transformed: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F33B4527027; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:NoSuchBucket: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F33B4527027; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:263)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 18667F33B4527027; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5414)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5408)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)\n\t... 49 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":28,"name":"transform"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1658,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T21:58:12.554895","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T21:58:12.555201","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T21:58:13.783138Z","level":"error","event":"25/09/18 21:58:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:58:13.925551Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:58:13.925822Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:58:14.542071Z","level":"error","event":"25/09/18 21:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:58:15.937071Z","level":"error","event":"25/09/18 21:58:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.186852Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187023Z","level":"info","event":"|      name|bestTrophies|         currentDeck|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187099Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187158Z","level":"info","event":"|RL|        9878|[{0, 3, 1, {https...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187213Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187278Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187339Z","level":"info","event":"Transformacin completada ","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T21:58:19.187029","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-09-18T22:02:37.221806","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:02:37.222227","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:02:38.576893Z","level":"error","event":"25/09/18 22:02:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:02:38.772935Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:02:38.773198Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:02:40.834514Z","level":"error","event":"25/09/18 22:02:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.065649Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.066051Z","level":"info","event":"|      name|bestTrophies|         currentDeck|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.066165Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.066241Z","level":"info","event":"|RL|        9878|[{0, 3, 1, {https...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.066310Z","level":"info","event":"+----------+------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:44.066372Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:45.133556Z","level":"info","event":"Transformacin completada ","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:02:45.133672","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-09-18T22:27:56.041522","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:27:56.041775","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:27:56.859722Z","level":"error","event":"25/09/18 22:27:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:27:56.960030Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:27:56.960175Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:27:57.433573Z","level":"error","event":"25/09/18 22:27:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:27:58.318962Z","level":"error","event":"25/09/18 22:27:58 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.998444Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.998685Z","level":"info","event":"|       tag|      name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.998818Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.998944Z","level":"info","event":"|#2PPCJ0UUP|RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.999068Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.999164Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:28:00.998659","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"ValueError","exc_value":"too many values to unpack (expected 2)","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":33,"name":"transform"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T22:31:28.955482","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:31:28.955735","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:31:29.813580Z","level":"error","event":"25/09/18 22:31:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:31:29.906511Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:31:29.906651Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:31:31.348635Z","level":"error","event":"25/09/18 22:31:31 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351290Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351492Z","level":"info","event":"|       tag|      name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351577Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351642Z","level":"info","event":"|#2PPCJ0UUP|RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351705Z","level":"info","event":"+----------+----------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.351766Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537103Z","level":"info","event":"+----------+----------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537355Z","level":"info","event":"|       tag|      name|exp_level|best_trophies|        current_deck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537488Z","level":"info","event":"+----------+----------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537596Z","level":"info","event":"|#2PPCJ0UUP|RL|       46|         9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537700Z","level":"info","event":"+----------+----------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537794Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537888Z","level":"info","event":"Transformacin completada ","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:31:34.537373","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-09-18T22:45:08.705626","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:45:08.705940","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:45:09.611643Z","level":"error","event":"25/09/18 22:45:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:45:09.708407Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:45:09.708657Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:45:11.246839Z","level":"error","event":"25/09/18 22:45:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.218985Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219179Z","level":"info","event":"|       tag|         name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219280Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219380Z","level":"info","event":"| #VP9GJYQ2|     xAlee |      70|        8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219461Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|      70|       10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219536Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|      44|        9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219612Z","level":"info","event":"|#2PPCJ0UUP|   RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219683Z","level":"info","event":"|#22GPGCVCV|      Diego02|      53|        9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219752Z","level":"info","event":"|#2L8JG2GRJ|         Shay|      40|        9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219828Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.219914Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:45:14.283269","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"NameError","exc_value":"name 'col' is not defined","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":48,"name":"transform"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T22:48:20.464191","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:48:20.464727","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:48:21.552281Z","level":"error","event":"25/09/18 22:48:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:48:21.663064Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:48:21.663255Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:48:22.170151Z","level":"error","event":"25/09/18 22:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:48:23.178409Z","level":"error","event":"25/09/18 22:48:23 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068493Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068643Z","level":"info","event":"|       tag|         name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068711Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068767Z","level":"info","event":"| #VP9GJYQ2|     xAlee |      70|        8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068819Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|      70|       10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068863Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|      44|        9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068913Z","level":"info","event":"|#2PPCJ0UUP|   RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.068966Z","level":"info","event":"|#22GPGCVCV|      Diego02|      53|        9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.069013Z","level":"info","event":"|#2L8JG2GRJ|         Shay|      40|        9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.069058Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.069102Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:48:26.108573","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"UnboundLocalError","exc_value":"cannot access local variable 'df_clans_transformed' where it is not associated with a value","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":49,"name":"transform"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T22:53:47.620135","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:53:47.620437","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:53:48.613851Z","level":"error","event":"25/09/18 22:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:53:48.708344Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:53:48.708495Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:53:49.155175Z","level":"error","event":"25/09/18 22:53:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:53:50.148217Z","level":"error","event":"25/09/18 22:53:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253497Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253676Z","level":"info","event":"|       tag|         name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253762Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253841Z","level":"info","event":"| #VP9GJYQ2|     xAlee |      70|        8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253921Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|      70|       10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.253996Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|      44|        9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.254065Z","level":"info","event":"|#2PPCJ0UUP|   RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.254133Z","level":"info","event":"|#22GPGCVCV|      Diego02|      53|        9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.254196Z","level":"info","event":"|#2L8JG2GRJ|         Shay|      40|        9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.254257Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.254320Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484140Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484325Z","level":"info","event":"|       tag|         name|exp_level|best_trophies|        current_deck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484442Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484529Z","level":"info","event":"| #VP9GJYQ2|     xAlee |       70|         8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484605Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|       70|        10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484675Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|       44|         9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484744Z","level":"info","event":"|#2PPCJ0UUP|   RL|       46|         9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484812Z","level":"info","event":"|#22GPGCVCV|      Diego02|       53|         9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484877Z","level":"info","event":"|#2L8JG2GRJ|         Shay|       40|         9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484941Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.485006Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:53:53.484209","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"NameError","exc_value":"name 'df_clans_trasnformed' is not defined","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/etl/scripts/transform_data.py","lineno":52,"name":"transform"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-09-18T22:59:35.999598","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-18T22:59:35.999947","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/cr_ingestion_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-18T22:59:36.915935Z","level":"error","event":"25/09/18 22:59:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:59:37.001964Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:59:37.002141Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:59:38.326818Z","level":"error","event":"25/09/18 22:59:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091575Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091688Z","level":"info","event":"|       tag|         name|expLevel|bestTrophies|         currentDeck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091719Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091747Z","level":"info","event":"| #VP9GJYQ2|     xAlee |      70|        8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091773Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|      70|       10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091799Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|      44|        9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091822Z","level":"info","event":"|#2PPCJ0UUP|   RL|      46|        9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091844Z","level":"info","event":"|#22GPGCVCV|      Diego02|      53|        9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091865Z","level":"info","event":"|#2L8JG2GRJ|         Shay|      40|        9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091886Z","level":"info","event":"+----------+-------------+--------+------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.091908Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310418Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310601Z","level":"info","event":"|       tag|         name|exp_level|best_trophies|        current_deck|                clan|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310703Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310797Z","level":"info","event":"| #VP9GJYQ2|     xAlee |       70|         8544|[{0, 3, 1, {https...|{16000161, Calala...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310876Z","level":"info","event":"|#G9YV9GR8R|Mohamed Light|       70|        10000|[{0, 4, 1, {https...|{16000002, Hey, #...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.310947Z","level":"info","event":"|#C0RL8CVCR|  zelarayann_|       44|         9444|[{0, 7, 1, {https...|{16000024, Los be...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.311015Z","level":"info","event":"|#2PPCJ0UUP|   RL|       46|         9878|[{0, 3, 1, {https...|{16000013, cordob...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.311085Z","level":"info","event":"|#22GPGCVCV|      Diego02|       53|         9837|[{0, 3, 1, {https...|                NULL|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.311150Z","level":"info","event":"|#2L8JG2GRJ|         Shay|       40|         9322|[{0, 3, 1, {https...|{16000000, HotPoc...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.311215Z","level":"info","event":"+----------+-------------+---------+-------------+--------------------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.311278Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534063Z","level":"info","event":"+---------+---------------+----------+-----------------+-----------------+-------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534239Z","level":"info","event":"|      tag|           name|clan_score|clan_war_trophies|required_trophies|members|         member_list|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534323Z","level":"info","event":"+---------+---------------+----------+-----------------+-----------------+-------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534406Z","level":"info","event":"| #2L80YUL|    cordoba arg|     97007|             2725|             8000|     50|[{{54000031, Lege...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534478Z","level":"info","event":"|#Q0U2PLGU|SupremacyLeague|     76837|              509|                0|     35|[{{54000018, Clas...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534546Z","level":"info","event":"|#2L8PCVP0|HotPocket Heros|     71637|             1670|             5000|     33|[{{54000031, Lege...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534611Z","level":"info","event":"|#QCV8JQVR|      Los bepis|     56134|              419|             1000|     27|[{{54000020, Valk...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534674Z","level":"info","event":"+---------+---------------+----------+-----------------+-----------------+-------+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534740Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534808Z","level":"info","event":"Transformacin completada ","chan":"stdout","logger":"task"}
{"timestamp":"2025-09-18T22:59:41.534148","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
